# ===== 运行与基础配置 =====
base_model: /root/autodl-tmp/LLM_RL_basic/weights/Qwen3-1p7B
output_dir: weights/grpo-qwen3-1p7b
reward_func: code/reward_math_verify.py
logging: tensorboard
dtype: bfloat16

# ===== Prompt 模板 =====
prompt_template: |  # GPT-4 tokenizer 49 token
  Please reason step by step. 
  At the very end, output ONLY the final numeric/closed-form answer in LaTeX as \boxed{{...}}. 
  Do not add any text after the box.

  Problem:
  {problem}

# ===== 数据路径（含输出） =====
train_parquet: /root/autodl-tmp/LLM_RL_basic/data/deepscaler_20.parquet   ###
test_parquet: /root/autodl-tmp/LLM_RL_basic/data/aime2025-one.parquet     ###
rollout_dir: /root/autodl-tmp/LLM_RL_basic/outputs/rollout

# ===== 测试，Checkpoint =====
val_bef_train: false
test_steps: 2
save_steps: 2                # 多少iteration save一次
max_actor_ckpt_to_keep: 1
max_actor_model_to_keep: 5

# ===== 分布式参数 =====
tp_size:  1 #4
n_gpus: 1  #4                     # 一机器多少GPU
nnodes: 1                     # 多少机器

# ===== 文本长度与 vLLM 显存 =====
max_prompt_len: 3450
max_resp_len: 3000 #30000           # 两者加起来 < 2^15
mem_utilz: 0.375  # 18        # vllm给kvcache的存储 

# ===== 训练超参（批次/迭代） =====
total_epochs: 1       
train_batch_size: 2           # 2, 丢给 rollout 多少 prompt
ppo_mini_batch_size: 2        # 2, 训练时以多少 prompt 的累积做一次参数更新
ppo_micro_batch_per_gpu: 1    # 每次实际前向/反向训多少个 trajectory（梯度累计）
ppo_epochs: 3                 # ppo一个数据重复优化几次
rollout_n: 8                  # 8, 每个prompt生成n个样本（GRPO组大小）
learning_rate: 2e-7           # Archer 训练 batch size 32, ppo epochs=1 是 1e-6，我的更少再除以 2*3
log_prob_micro_bs: 2          # 计算 log prob 时多少 prompt 一起计算 (entropy 显存大)
data_shuffle: False           # train_dataloader 的顺序是否混乱

# trajectory_len/1000 * 0.12 (Qwen3-1p7B) * train_batch_size * rollout_n / tp_size < MEM * mem_utilz

# ===== 算法与正则（GRPO / KL / 策略） =====
adv_estimator: grpo
is_dapo: true                 # 是否丢弃全对全错
use_kl: true
clip_ratio_low:  0.2
clip_ratio_high: 0.28
temp: 0.7                     # Qwen3-1p7b 建议 0.6, Qwen2.5 (ProRL 1.2, Archer 1.0)
attn: flash_attention_2

# ===== 其他 =====
seed: 42
